\section{Evaluation}
\label{sec:evaluation}

% earlier test ; may remove it?
Following the development phases of \boatvtwo, 4 test were conducted in order to evaluate the tool. The aim of each test was to assess a different aspect of \boatvtwo. 

\subsection{Version Comparison}
The first test was carried out in June 2022 to compare \boatvone\ and \boatvtwo. As part of this test, two 10-sentence sets were created. Each set contained unique sentences with corresponding lengths. The reason behind this choice was to eliminate potential confounders so that two versions of the tool could be compared in terms of annotator speed, responsiveness, and various other metrics. 

A single annotator used \boatvone\ to annotate the first set, then used \boatvtwo to annotate the second set.
During both of the annotation processes, an observer took notes and timed how long it took to complete each sentence on each tool.
There was a noticeable speedup (approximately 30\%) using \boatvtwo.
Among the new features that were most appreciated are auto-completion, condensed dependency tree representation, a significant reduction in scrolling, keyword search, and search by morphological features.
The non-search-related features proved to be instrumental in retaining focus.

\subsection{User Testing}
After version comparison, several test cases were carefully designed for user testing in July 2022.
These cases included testing various functionalities of the tool, such as creating annotator accounts, creating and uploading treebanks, annotating both simple\footnote{Simple sentence annotation refers to sentences with less than 10 tokens.} and complex sentences\footnote{Complex sentence annotation included long sentences which also required splitting lemmas.}, splitting and merging lemmas, changing preexisting annotation tags, viewing different dependency graphs, and searching in previously created annotations.

The goal of the test was to fully assess the responsiveness and functionality of \boatvtwo.
Thus, the cases covered both newly introduced functions like choosing different dependency graph styles and old functions like lemma splitting.
Three experienced linguist annotators carried out the user tests on different operating systems and browsers (Safari~\cite{safari} on macOS~\cite{macos}, Chrome~\cite{chrome} on Windows~\cite{windows}), all on desktop devices.
Following the tests, they filled out questionnaires to give feedback on whether they were able to carry out the annotations and operations they were expected to.
In addition, they reported any errors or issues they came across during the testing.
The test results were mostly matching what the expected results were.
The feedback provided by the participants was referred to while improving the tool further and making it more robust.

\subsection{Automated Testing}
After user testing, we wanted to construct automated testing scripts in order that the development does not produce unintended results, such as preventing a feature from working expectedly.
For this, we have designed and tested with several test cases via Selenium~\cite{selenium} in September 2022.
In these tests, we aimed to test if the tool's interface works as expected in various use cases, leveraging assertions in the tests.
These use cases included logging in, creating and deleting treebanks, and searching sentences.
The actual results of these tests were matching the expected results.
We plan to use these automated testing scripts when we update the corresponding web pages in these tests so that the development continues to improve the tool.

\subsection{Learnability and Intuitiveness}
The final round of testing targeted the learnability and intuitiveness of \boatvtwo.
For this purpose, a test case similar to the previous user testing was designed and carried out in December 2022.
This test case included numerous functionalities of the tool, such as creating and logging into a user account, creating and uploading a treebank, annotating parts of simple and complex sentences, changing preexisting annotations, merging lemmas, and searching in current and previously annotated treebanks.

% One undergraduate and one graduate students from the Department of Linguistics at Boğaziçi University  participated in this test.
One undergraduate and one graduate students~\cite{anon} participated in the test.
Neither of the participants had done any annotation prior to testing.
Before they started the test, they were introduced to the tool and the \ud\ framework by an experienced linguist for the first time.
They were taught how to use the tool and do annotations, and then they were given the dataset to be annotated.
They did the tests on the same device and browser they received the training on (Safari on a desktop macOS device). 

As they carried out the tests, the participants were observed by the trainer who also took notes regarding the tests.
The participants were able to ask questions and get directions from the trainer regarding both the \ud\ framework and the annotation tool.
After testing, the participants were asked to fill out a questionnaire regarding their experience.
Their feedback and notes taken by the expert during the tests were considered together by the development team.